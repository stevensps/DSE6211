nn_model %>% compile(optimizer = optimizer_adam(learning_rate = 0.01),
loss = "mean_squared_error",
metrics = "mean_absolute_error")
nn_model_training <- nn_model %>% fit(x = mtcars_x,
y = mtcars_y,
epochs = 10000,
verbose = FALSE)
plot(nn_model_training)
get_weights(nn_model)
prediction1 <- predict(nn_model, array(c(6, 350, 125), dim = c(1, 3)))
prediction2 <- predict(nn_model, array(c(8, 250, 200), dim = c(1, 3)))
print(prediction1)
print(prediction2)
library(reticulate)
library(tensorflow)
library(keras)
clear_session(free_memory = TRUE)
# Chunk 1
library(reticulate)
library(tensorflow)
library(keras)
use_virtualenv("my_tf_workspace", required = TRUE)
mtcars <- mtcars
mtcars_x <- mtcars[, c("cyl", "disp", "hp")]
mtcars_x <- array(data = unlist(mtcars_x), dim = c(nrow(mtcars), 3),
dimnames = list(rownames(mtcars),
colnames(mtcars_x)))
mtcars_y <- mtcars[, "mpg"]
set.seed(42)
nn_model <- keras_model_sequential() %>%
layer_dense(units = 1, input_shape = 3, activation = "linear")
nn_model
#clear_session(free_memory = TRUE)
nn_model <- keras_model_sequential() %>%
layer_dense(units = 4, input_shape = 3, activation = "relu") %>%
layer_dense(units = 1, activation = "linear")
nn_model
nn_model %>% compile(optimizer = optimizer_adam(learning_rate = 0.01),
loss = "mean_squared_error",
metrics = "mean_absolute_error")
nn_model_training <- nn_model %>% fit(x = mtcars_x,
y = mtcars_y,
epochs = 10000,
verbose = FALSE)
plot(nn_model_training)
get_weights(nn_model)
prediction1 <- predict(nn_model, array(c(6, 350, 125), dim = c(1, 3)))
clear_session(free_memory = TRUE)
prediction2 <- predict(nn_model, array(c(8, 250, 200), dim = c(1, 3)))
print(prediction1)
print(prediction2)
# Chunk 1
library(reticulate)
library(tensorflow)
library(keras)
use_virtualenv("my_tf_workspace", required = TRUE)
mtcars <- mtcars
mtcars_x <- mtcars[, c("cyl", "disp", "hp")]
mtcars_x <- array(data = unlist(mtcars_x), dim = c(nrow(mtcars), 3),
dimnames = list(rownames(mtcars),
colnames(mtcars_x)))
mtcars_y <- mtcars[, "mpg"]
set.seed(42)
nn_model <- keras_model_sequential() %>%
layer_dense(units = 1, input_shape = 3, activation = "linear")
nn_model
#clear_session(free_memory = TRUE)
nn_model <- keras_model_sequential() %>%
layer_dense(units = 4, input_shape = 3, activation = "relu") %>%
layer_dense(units = 1, activation = "linear")
nn_model
nn_model %>% compile(optimizer = optimizer_adam(learning_rate = 0.01),
loss = "mean_squared_error",
metrics = "mean_absolute_error")
nn_model_training <- nn_model %>% fit(x = mtcars_x,
y = mtcars_y,
epochs = 10000,
verbose = FALSE)
plot(nn_model_training)
get_weights(nn_model)
prediction1 <- predict(nn_model, array(c(6, 350, 125), dim = c(1, 3)))
prediction2 <- predict(nn_model, array(c(8, 250, 200), dim = c(1, 3)))
print(prediction1)
print(prediction2)
# Chunk 1
library(reticulate)
library(tensorflow)
library(keras3)
use_virtualenv("my_tf_workspace", required = TRUE)
mtcars <- mtcars
mtcars_x <- mtcars[, c("cyl", "disp", "hp")]
mtcars_x <- array(data = unlist(mtcars_x), dim = c(nrow(mtcars), 3),
dimnames = list(rownames(mtcars),
colnames(mtcars_x)))
mtcars_y <- mtcars[, "mpg"]
set.seed(42)
nn_model <- keras_model_sequential() %>%
layer_dense(units = 1, input_shape = 3, activation = "linear")
nn_model
#clear_session(free_memory = TRUE)
nn_model <- keras_model_sequential() %>%
layer_dense(units = 4, input_shape = 3, activation = "relu") %>%
layer_dense(units = 1, activation = "linear")
nn_model
nn_model %>% compile(optimizer = optimizer_adam(learning_rate = 0.01),
loss = "mean_squared_error",
metrics = "mean_absolute_error")
nn_model_training <- nn_model %>% fit(x = mtcars_x,
y = mtcars_y,
epochs = 10000,
verbose = FALSE)
plot(nn_model_training)
get_weights(nn_model)
prediction1 <- predict(nn_model, array(c(6, 350, 125), dim = c(1, 3)))
prediction2 <- predict(nn_model, array(c(8, 250, 200), dim = c(1, 3)))
print(prediction1)
print(prediction2)
library(dplyr)
library(caret)
data <- read.csv("lab_4_data.csv")
getwd()
setwd("C:\\Users\\steve\\OneDrive\\Documents\\School\\DSE6211\\Week4")
data <- read.csv("lab_4_data.csv")
training_ind <- createDataPartition(data$lodgepole_pine,
p = 0.75,
list = FALSE,
times = 1)
training_set <- data[training_ind, ]
test_set <- data[-training_ind, ]
unique(training_set$wilderness_area)
unique(training_set$soil_type)
top_20_soil_types <- training_set %>%
group_by(soil_type) %>%
summarise(count = n()) %>%
arrange(desc(count)) %>%
select(soil_type) %>%
top_n(20)
training_set$soil_type <- ifelse(training_set$soil_type %in% top_20_soil_types$soil_type,
training_set$soil_type,
"other")
training_set$wilderness_area <- factor(training_set$wilderness_area)
training_set$soil_type <- factor(training_set$soil_type)
class(training_set$wilderness_area)
class(training_set$soil_type)
levels(training_set$wilderness_area)
levels(training_set$soil_type)
onehot_encoder <- dummyVars(~ wilderness_area + soil_type,
training_set[, c("wilderness_area", "soil_type")],
levelsOnly = TRUE,
fullRank = TRUE)
onehot_enc_training <- predict(onehot_encoder,
training_set[, c("wilderness_area", "soil_type")])
training_set <- cbind(training_set, onehot_enc_training)
test_set$soil_type <- ifelse(test_set$soil_type %in% top_20_soil_types$soil_type,
test_set$soil_type,
"other")
test_set$wilderness_area <- factor(test_set$wilderness_area)
test_set$soil_type <- factor(test_set$soil_type)
onehot_enc_test <- predict(onehot_encoder, test_set[, c("wilderness_area", "soil_type")])
test_set <- cbind(test_set, onehot_enc_test)
test_set[, -c(11:13)] <- scale(test_set[, -c(11:13)],
center = apply(training_set[, -c(11:13)], 2, mean),
scale = apply(training_set[, -c(11:13)], 2, sd))
training_set[, -c(11:13)] <- scale(training_set[, -c(11:13)])
training_features <- array(data = unlist(training_set[, -c(11:13)]),
dim = c(nrow(training_set), 33))
training_labels <- array(data = unlist(training_set[, 13]),
dim = c(nrow(training_set)))
test_features <- array(data = unlist(test_set[, -c(11:13)]),
dim = c(nrow(test_set), 33))
test_labels <- array(data = unlist(test_set[, 13]),
dim = c(nrow(test_set)))
library(reticulate)
library(tensorflow)
library(keras3)
use_virtualenv("my_tf_workspace")
model1 <- keras_model_sequential() %>%
layer_dense(units = 20, activation = "relu") %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
compile(model1,
optimizer = "rmsprop",
loss = "binary_crossentropy",
metrics = "accuracy")
history1 <- fit(model1, training_features, training_labels,
epochs = 100, batch_size = 512, validation_split = 0.33)
plot(history1)
predictions1 <- predict(model1, test_features)
head(predictions1, 10)
predicted_class1 <- (predictions1[, 1] >= 0.5) * 1
head(predicted_class1, 10)
model2 <- keras_model_sequential() %>%
layer_dense(units = 50, activation = "relu") %>%
layer_dense(units = 25, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
compile(model2,
optimizer = "rmsprop",
loss = "binary_crossentropy",
metrics = "accuracy")
history2 <- fit(model2, training_features, training_labels,
epochs = 100, batch_size = 512, validation_split = 0.33)
plot(history2)
predictions2 <- predict(model2, test_features)
head(predictions, 10)
head(predictions2, 10)
predicted_class2 <- (predictions2[, 1] >= 0.5) * 1
head(predicted_class2, 10)
results1 <- model1 %>% evaluate(test_features, test_labels)
results1
results2 <- model2 %>% evaluate(test_features, test_labels)
results2
# Chunk 1
library(dplyr)
library(caret)
setwd("C:\\Users\\steve\\OneDrive\\Documents\\School\\DSE6211\\Week4")
data <- read.csv("lab_4_data.csv")
set.seed(42)
training_ind <- createDataPartition(data$lodgepole_pine,
p = 0.75,
list = FALSE,
times = 1)
training_set <- data[training_ind, ]
test_set <- data[-training_ind, ]
unique(training_set$wilderness_area)
unique(training_set$soil_type)
top_20_soil_types <- training_set %>%
group_by(soil_type) %>%
summarise(count = n()) %>%
arrange(desc(count)) %>%
select(soil_type) %>%
top_n(20)
training_set$soil_type <- ifelse(training_set$soil_type %in% top_20_soil_types$soil_type,
training_set$soil_type,
"other")
training_set$wilderness_area <- factor(training_set$wilderness_area)
training_set$soil_type <- factor(training_set$soil_type)
class(training_set$wilderness_area)
class(training_set$soil_type)
levels(training_set$wilderness_area)
levels(training_set$soil_type)
onehot_encoder <- dummyVars(~ wilderness_area + soil_type,
training_set[, c("wilderness_area", "soil_type")],
levelsOnly = TRUE,
fullRank = TRUE)
onehot_enc_training <- predict(onehot_encoder,
training_set[, c("wilderness_area", "soil_type")])
training_set <- cbind(training_set, onehot_enc_training)
test_set$soil_type <- ifelse(test_set$soil_type %in% top_20_soil_types$soil_type,
test_set$soil_type,
"other")
test_set$wilderness_area <- factor(test_set$wilderness_area)
test_set$soil_type <- factor(test_set$soil_type)
onehot_enc_test <- predict(onehot_encoder, test_set[, c("wilderness_area", "soil_type")])
test_set <- cbind(test_set, onehot_enc_test)
test_set[, -c(11:13)] <- scale(test_set[, -c(11:13)],
center = apply(training_set[, -c(11:13)], 2, mean),
scale = apply(training_set[, -c(11:13)], 2, sd))
training_set[, -c(11:13)] <- scale(training_set[, -c(11:13)])
training_features <- array(data = unlist(training_set[, -c(11:13)]),
dim = c(nrow(training_set), 33))
training_labels <- array(data = unlist(training_set[, 13]),
dim = c(nrow(training_set)))
test_features <- array(data = unlist(test_set[, -c(11:13)]),
dim = c(nrow(test_set), 33))
test_labels <- array(data = unlist(test_set[, 13]),
dim = c(nrow(test_set)))
library(reticulate)
library(tensorflow)
library(keras3)
use_virtualenv("my_tf_workspace")
model1 <- keras_model_sequential() %>%
layer_dense(units = 20, activation = "relu") %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
compile(model1,
optimizer = "rmsprop",
loss = "binary_crossentropy",
metrics = "accuracy")
set.seed(42)
history1 <- fit(model1, training_features, training_labels,
epochs = 100, batch_size = 512, validation_split = 0.33)
plot(history1)
set.seed(42)
predictions1 <- predict(model1, test_features)
head(predictions1, 10)
predicted_class1 <- (predictions1[, 1] >= 0.5) * 1
head(predicted_class1, 10)
# Chunk 2
model2 <- keras_model_sequential() %>%
layer_dense(units = 50, activation = "relu") %>%
layer_dense(units = 25, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
compile(model2,
optimizer = "rmsprop",
loss = "binary_crossentropy",
metrics = "accuracy")
set.seed(42)
history2 <- fit(model2, training_features, training_labels,
epochs = 100, batch_size = 512, validation_split = 0.33)
plot(history2)
set.seed(42)
predictions2 <- predict(model2, test_features)
head(predictions2, 10)
predicted_class2 <- (predictions2[, 1] >= 0.5) * 1
head(predicted_class2, 10)
# Chunk 3
results1 <- model1 %>% evaluate(test_features, test_labels)
results1
results2 <- model2 %>% evaluate(test_features, test_labels)
results2
# Chunk 1
library(dplyr)
library(caret)
setwd("C:\\Users\\steve\\OneDrive\\Documents\\School\\DSE6211\\Week4")
data <- read.csv("lab_4_data.csv")
set.seed(42)
training_ind <- createDataPartition(data$lodgepole_pine,
p = 0.75,
list = FALSE,
times = 1)
training_set <- data[training_ind, ]
test_set <- data[-training_ind, ]
unique(training_set$wilderness_area)
unique(training_set$soil_type)
top_20_soil_types <- training_set %>%
group_by(soil_type) %>%
summarise(count = n()) %>%
arrange(desc(count)) %>%
select(soil_type) %>%
top_n(20)
training_set$soil_type <- ifelse(training_set$soil_type %in% top_20_soil_types$soil_type,
training_set$soil_type,
"other")
training_set$wilderness_area <- factor(training_set$wilderness_area)
training_set$soil_type <- factor(training_set$soil_type)
class(training_set$wilderness_area)
class(training_set$soil_type)
levels(training_set$wilderness_area)
levels(training_set$soil_type)
onehot_encoder <- dummyVars(~ wilderness_area + soil_type,
training_set[, c("wilderness_area", "soil_type")],
levelsOnly = TRUE,
fullRank = TRUE)
onehot_enc_training <- predict(onehot_encoder,
training_set[, c("wilderness_area", "soil_type")])
training_set <- cbind(training_set, onehot_enc_training)
test_set$soil_type <- ifelse(test_set$soil_type %in% top_20_soil_types$soil_type,
test_set$soil_type,
"other")
test_set$wilderness_area <- factor(test_set$wilderness_area)
test_set$soil_type <- factor(test_set$soil_type)
onehot_enc_test <- predict(onehot_encoder, test_set[, c("wilderness_area", "soil_type")])
test_set <- cbind(test_set, onehot_enc_test)
test_set[, -c(11:13)] <- scale(test_set[, -c(11:13)],
center = apply(training_set[, -c(11:13)], 2, mean),
scale = apply(training_set[, -c(11:13)], 2, sd))
training_set[, -c(11:13)] <- scale(training_set[, -c(11:13)])
training_features <- array(data = unlist(training_set[, -c(11:13)]),
dim = c(nrow(training_set), 33))
training_labels <- array(data = unlist(training_set[, 13]),
dim = c(nrow(training_set)))
test_features <- array(data = unlist(test_set[, -c(11:13)]),
dim = c(nrow(test_set), 33))
test_labels <- array(data = unlist(test_set[, 13]),
dim = c(nrow(test_set)))
library(reticulate)
library(tensorflow)
library(keras3)
use_virtualenv("my_tf_workspace")
model1 <- keras_model_sequential() %>%
layer_dense(units = 20, activation = "relu") %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
compile(model1,
optimizer = "rmsprop",
loss = "binary_crossentropy",
metrics = "accuracy")
set.seed(42)
history1 <- fit(model1, training_features, training_labels,
epochs = 100, batch_size = 512, validation_split = 0.33)
plot(history1)
set.seed(42)
predictions1 <- predict(model1, test_features)
head(predictions1, 10)
predicted_class1 <- (predictions1[, 1] >= 0.5) * 1
head(predicted_class1, 10)
# Chunk 2
model2 <- keras_model_sequential() %>%
layer_dense(units = 50, activation = "relu") %>%
layer_dense(units = 25, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
compile(model2,
optimizer = "rmsprop",
loss = "binary_crossentropy",
metrics = "accuracy")
set.seed(42)
history2 <- fit(model2, training_features, training_labels,
epochs = 100, batch_size = 512, validation_split = 0.33)
plot(history2)
set.seed(42)
predictions2 <- predict(model2, test_features)
head(predictions2, 10)
predicted_class2 <- (predictions2[, 1] >= 0.5) * 1
head(predicted_class2, 10)
# Chunk 3
set.seed(42)
results1 <- model1 %>% evaluate(test_features, test_labels)
results1
results2 <- model2 %>% evaluate(test_features, test_labels)
results2
# Chunk 1
library(dplyr)
library(caret)
setwd("C:\\Users\\steve\\OneDrive\\Documents\\School\\DSE6211\\Week4")
data <- read.csv("lab_4_data.csv")
set.seed(42)
training_ind <- createDataPartition(data$lodgepole_pine,
p = 0.75,
list = FALSE,
times = 1)
training_set <- data[training_ind, ]
test_set <- data[-training_ind, ]
unique(training_set$wilderness_area)
unique(training_set$soil_type)
top_20_soil_types <- training_set %>%
group_by(soil_type) %>%
summarise(count = n()) %>%
arrange(desc(count)) %>%
select(soil_type) %>%
top_n(20)
training_set$soil_type <- ifelse(training_set$soil_type %in% top_20_soil_types$soil_type,
training_set$soil_type,
"other")
training_set$wilderness_area <- factor(training_set$wilderness_area)
training_set$soil_type <- factor(training_set$soil_type)
class(training_set$wilderness_area)
class(training_set$soil_type)
levels(training_set$wilderness_area)
levels(training_set$soil_type)
onehot_encoder <- dummyVars(~ wilderness_area + soil_type,
training_set[, c("wilderness_area", "soil_type")],
levelsOnly = TRUE,
fullRank = TRUE)
onehot_enc_training <- predict(onehot_encoder,
training_set[, c("wilderness_area", "soil_type")])
training_set <- cbind(training_set, onehot_enc_training)
test_set$soil_type <- ifelse(test_set$soil_type %in% top_20_soil_types$soil_type,
test_set$soil_type,
"other")
test_set$wilderness_area <- factor(test_set$wilderness_area)
test_set$soil_type <- factor(test_set$soil_type)
onehot_enc_test <- predict(onehot_encoder, test_set[, c("wilderness_area", "soil_type")])
test_set <- cbind(test_set, onehot_enc_test)
test_set[, -c(11:13)] <- scale(test_set[, -c(11:13)],
center = apply(training_set[, -c(11:13)], 2, mean),
scale = apply(training_set[, -c(11:13)], 2, sd))
training_set[, -c(11:13)] <- scale(training_set[, -c(11:13)])
training_features <- array(data = unlist(training_set[, -c(11:13)]),
dim = c(nrow(training_set), 33))
training_labels <- array(data = unlist(training_set[, 13]),
dim = c(nrow(training_set)))
test_features <- array(data = unlist(test_set[, -c(11:13)]),
dim = c(nrow(test_set), 33))
test_labels <- array(data = unlist(test_set[, 13]),
dim = c(nrow(test_set)))
library(reticulate)
library(tensorflow)
library(keras3)
use_virtualenv("my_tf_workspace")
set.seed(42)
model1 <- keras_model_sequential() %>%
layer_dense(units = 20, activation = "relu") %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
compile(model1,
optimizer = "rmsprop",
loss = "binary_crossentropy",
metrics = "accuracy")
set.seed(42)
history1 <- fit(model1, training_features, training_labels,
epochs = 100, batch_size = 512, validation_split = 0.33)
plot(history1)
set.seed(42)
predictions1 <- predict(model1, test_features)
head(predictions1, 10)
predicted_class1 <- (predictions1[, 1] >= 0.5) * 1
head(predicted_class1, 10)
# Chunk 2
set.seed(42)
model2 <- keras_model_sequential() %>%
layer_dense(units = 50, activation = "relu") %>%
layer_dense(units = 25, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
compile(model2,
optimizer = "rmsprop",
loss = "binary_crossentropy",
metrics = "accuracy")
set.seed(42)
history2 <- fit(model2, training_features, training_labels,
epochs = 100, batch_size = 512, validation_split = 0.33)
plot(history2)
set.seed(42)
predictions2 <- predict(model2, test_features)
head(predictions2, 10)
predicted_class2 <- (predictions2[, 1] >= 0.5) * 1
head(predicted_class2, 10)
# Chunk 3
set.seed(42)
results1 <- model1 %>% evaluate(test_features, test_labels)
results1
results2 <- model2 %>% evaluate(test_features, test_labels)
results2
