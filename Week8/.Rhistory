print(prediction1)
prediction2 <- predict(nn_model, array(c(8, 250, 200), dim = c(1, 3)))
print(prediction2)
clear_session(free_memory = TRUE
clear_session(free_memory = TRUE)
prediction2 <- predict(nn_model, array(c(8, 250, 200), dim = c(1, 3)))
print(prediction2)
clear_session(free_memory = TRUE)
prediction2 <- predict(nn_model, array(c(8, 250, 200), dim = c(1, 3)))
print(prediction2)
library(reticulate)
library(tensorflow)
library(keras)
install.packages("keras")
library(reticulate)
library(tensorflow)
library(keras)
clear_session(free_memory = TRUE)
use_virtualenv("my_tf_workspace", required = TRUE)
mtcars <- mtcars
mtcars_x <- mtcars[, c("cyl", "disp", "hp")]
mtcars_x <- array(data = unlist(mtcars_x), dim = c(nrow(mtcars), 3),
dimnames = list(rownames(mtcars),
colnames(mtcars_x)))
mtcars_y <- mtcars[, "mpg"]
nn_model <- keras_model_sequential() %>%
layer_dense(units = 1, input_shape = 3, activation = "linear")
nn_model
nn_model <- keras_model_sequential() %>%
layer_dense(units = 2, input_shape = 3, activation = "relu") %>%
layer_dense(units = 1, activation = "linear")
nn_model
nn_model %>% compile(optimizer = optimizer_adam(learning_rate = 0.2),
loss = "mean_squared_error",
metrics = "mean_absolute_error")
nn_model_training <- nn_model %>% fit(x = mtcars_x,
y = mtcars_y,
epochs = 250,
verbose = FALSE)
plot(nn_model_training)
get_weights(nn_model)
prediction1 <- predict(nn_model, array(c(6, 350, 125), dim = c(1, 3)))
prediction2 <- predict(nn_model, array(c(8, 250, 200), dim = c(1, 3)))
print(prediction1)
print(prediction2)
clear_session(free_memory = TRUE)
prediction2 <- predict(nn_model, array(c(8, 250, 200), dim = c(1, 3)))
print(prediction2)
library(reticulate)
library(tensorflow)
library(keras)
clear_session(free_memory = TRUE)
use_virtualenv("my_tf_workspace", required = TRUE)
mtcars <- mtcars
mtcars_x <- mtcars[, c("cyl", "disp", "hp")]
mtcars_x <- array(data = unlist(mtcars_x), dim = c(nrow(mtcars), 3),
dimnames = list(rownames(mtcars),
colnames(mtcars_x)))
mtcars_y <- mtcars[, "mpg"]
nn_model <- keras_model_sequential() %>%
layer_dense(units = 1, input_shape = 3, activation = "linear")
nn_model
nn_model <- keras_model_sequential() %>%
layer_dense(units = 4, input_shape = 3, activation = "relu") %>%
layer_dense(units = 1, activation = "linear")
nn_model
nn_model %>% compile(optimizer = optimizer_adam(learning_rate = 0.2),
loss = "mean_squared_error",
metrics = "mean_absolute_error")
nn_model_training <- nn_model %>% fit(x = mtcars_x,
y = mtcars_y,
epochs = 250,
verbose = FALSE)
plot(nn_model_training)
get_weights(nn_model)
prediction1 <- predict(nn_model, array(c(6, 350, 125), dim = c(1, 3)))
prediction2 <- predict(nn_model, array(c(8, 250, 200), dim = c(1, 3)))
print(prediction1)
print(prediction2)
clear_session(free_memory = TRUE)
use_virtualenv("my_tf_workspace", required = TRUE)
mtcars <- mtcars
mtcars_x <- mtcars[, c("cyl", "disp", "hp")]
mtcars_x <- array(data = unlist(mtcars_x), dim = c(nrow(mtcars), 3),
dimnames = list(rownames(mtcars),
colnames(mtcars_x)))
mtcars_y <- mtcars[, "mpg"]
nn_model <- keras_model_sequential() %>%
layer_dense(units = 1, input_shape = 3, activation = "linear")
nn_model
nn_model <- keras_model_sequential() %>%
layer_dense(units = 4, input_shape = 3, activation = "relu") %>%
layer_dense(units = 1, activation = "linear")
nn_model
nn_model %>% compile(optimizer = optimizer_adam(learning_rate = 0.01),
loss = "mean_squared_error",
metrics = "mean_absolute_error")
nn_model_training <- nn_model %>% fit(x = mtcars_x,
y = mtcars_y,
epochs = 10000,
verbose = FALSE)
plot(nn_model_training)
get_weights(nn_model)
prediction1 <- predict(nn_model, array(c(6, 350, 125), dim = c(1, 3)))
prediction2 <- predict(nn_model, array(c(8, 250, 200), dim = c(1, 3)))
print(prediction1)
print(prediction2)
library(reticulate)
library(tensorflow)
library(keras)
clear_session(free_memory = TRUE)
# Chunk 1
library(reticulate)
library(tensorflow)
library(keras)
use_virtualenv("my_tf_workspace", required = TRUE)
mtcars <- mtcars
mtcars_x <- mtcars[, c("cyl", "disp", "hp")]
mtcars_x <- array(data = unlist(mtcars_x), dim = c(nrow(mtcars), 3),
dimnames = list(rownames(mtcars),
colnames(mtcars_x)))
mtcars_y <- mtcars[, "mpg"]
set.seed(42)
nn_model <- keras_model_sequential() %>%
layer_dense(units = 1, input_shape = 3, activation = "linear")
nn_model
#clear_session(free_memory = TRUE)
nn_model <- keras_model_sequential() %>%
layer_dense(units = 4, input_shape = 3, activation = "relu") %>%
layer_dense(units = 1, activation = "linear")
nn_model
nn_model %>% compile(optimizer = optimizer_adam(learning_rate = 0.01),
loss = "mean_squared_error",
metrics = "mean_absolute_error")
nn_model_training <- nn_model %>% fit(x = mtcars_x,
y = mtcars_y,
epochs = 10000,
verbose = FALSE)
plot(nn_model_training)
get_weights(nn_model)
prediction1 <- predict(nn_model, array(c(6, 350, 125), dim = c(1, 3)))
clear_session(free_memory = TRUE)
prediction2 <- predict(nn_model, array(c(8, 250, 200), dim = c(1, 3)))
print(prediction1)
print(prediction2)
# Chunk 1
library(reticulate)
library(tensorflow)
library(keras)
use_virtualenv("my_tf_workspace", required = TRUE)
mtcars <- mtcars
mtcars_x <- mtcars[, c("cyl", "disp", "hp")]
mtcars_x <- array(data = unlist(mtcars_x), dim = c(nrow(mtcars), 3),
dimnames = list(rownames(mtcars),
colnames(mtcars_x)))
mtcars_y <- mtcars[, "mpg"]
set.seed(42)
nn_model <- keras_model_sequential() %>%
layer_dense(units = 1, input_shape = 3, activation = "linear")
nn_model
#clear_session(free_memory = TRUE)
nn_model <- keras_model_sequential() %>%
layer_dense(units = 4, input_shape = 3, activation = "relu") %>%
layer_dense(units = 1, activation = "linear")
nn_model
nn_model %>% compile(optimizer = optimizer_adam(learning_rate = 0.01),
loss = "mean_squared_error",
metrics = "mean_absolute_error")
nn_model_training <- nn_model %>% fit(x = mtcars_x,
y = mtcars_y,
epochs = 10000,
verbose = FALSE)
plot(nn_model_training)
get_weights(nn_model)
prediction1 <- predict(nn_model, array(c(6, 350, 125), dim = c(1, 3)))
prediction2 <- predict(nn_model, array(c(8, 250, 200), dim = c(1, 3)))
print(prediction1)
print(prediction2)
# Chunk 1
library(reticulate)
library(tensorflow)
library(keras3)
use_virtualenv("my_tf_workspace", required = TRUE)
mtcars <- mtcars
mtcars_x <- mtcars[, c("cyl", "disp", "hp")]
mtcars_x <- array(data = unlist(mtcars_x), dim = c(nrow(mtcars), 3),
dimnames = list(rownames(mtcars),
colnames(mtcars_x)))
mtcars_y <- mtcars[, "mpg"]
set.seed(42)
nn_model <- keras_model_sequential() %>%
layer_dense(units = 1, input_shape = 3, activation = "linear")
nn_model
#clear_session(free_memory = TRUE)
nn_model <- keras_model_sequential() %>%
layer_dense(units = 4, input_shape = 3, activation = "relu") %>%
layer_dense(units = 1, activation = "linear")
nn_model
nn_model %>% compile(optimizer = optimizer_adam(learning_rate = 0.01),
loss = "mean_squared_error",
metrics = "mean_absolute_error")
nn_model_training <- nn_model %>% fit(x = mtcars_x,
y = mtcars_y,
epochs = 10000,
verbose = FALSE)
plot(nn_model_training)
get_weights(nn_model)
prediction1 <- predict(nn_model, array(c(6, 350, 125), dim = c(1, 3)))
prediction2 <- predict(nn_model, array(c(8, 250, 200), dim = c(1, 3)))
print(prediction1)
print(prediction2)
gc()
set.seed(123)
library(ggplot2)
true_relationship <- function(x) { return(6*xˆ3 + 6*xˆ2 - 12*x) }
x <- seq(-3, 2, by = 0.1)
f <- true_relationship(x)
set.seed(123)
library(ggplot2)
true_relationship <- function(x) { return(6*x^3 + 6*x^2 - 12^x) }
x <- seq(-3, 2, by = 0.1)
f <- true_relationship(x)
ggplot() + geom_line(aes(x = x, y = f), color = "black")
observations <- f + rnorm(length(x), mean = 0, sd = 15)
model1 <- lm(observations ~ poly(x, 1))
predictions1 <- predict(model1, newdata = data.frame(x = x))
model25 <- lm(observations ~ poly(x, 25))
predictions25 <- predict(model25, newdata = data.frame(x = x))
data <- data.frame(x = x,
f = f,
observations = observations,
lm = predictions1,
pm = predictions25)
View(data)
ggplot(data, aes(x = x)) +
geom_line(aes(y = f), color = "black") +
geom_point(aes(y = observations), color = "blue", shape = 1) +
geom_line(aes(y = lm), color = "red", linetype = "solid") +
geom_line(aes(y = pm), color = "orange", linetype = "solid") +
geom_point(aes(x = 1, y = data[x == 1, "lm"]), color = "red", shape=2) +
geom_point(aes(x = 1, y = data[x == 1, "pm"]), color = "orange", shape=2)
observations_new <- f + rnorm(length(x), mean = 0, sd = 15)
model1 <- lm(observations_new ~ poly(x, 1))
predictions1 <- predict(model1, newdata = data.frame(x = x))
model25 <- lm(observations_new ~ poly(x, 25))
predictions25 <- predict(model25, newdata = data.frame(x = x))
data <- data.frame(x = x,
f = f,
observations = observations_new,
lm = predictions1,
pm = predictions25)
ggplot(data, aes(x = x)) +
geom_line(aes(y = f), color = "black") +
geom_point(aes(y = observations), color = "blue", shape = 1) +
geom_line(aes(y = lm), color = "red", linetype = "solid") +
geom_line(aes(y = pm), color = "orange", linetype = "solid") +
geom_point(aes(x = 1, y = data[x == 1, "lm"]), color = "red", shape = 2) +
geom_point(aes(x = 1, y = data[x == 1, "pm"]), color = "orange", shape = 2)
results1 <- data.frame(x = 1, f_pred = 0)
for (i in 1:500) {
x <- seq(-3, 2, by = 0.1)
f <- true_relationship(x)
temp_observations <- f + rnorm(length(x), mean=0, sd=15)
model1 <- lm(temp_observations ~ poly(x, 1))
results1[i, 1] <- 1
results1[i, 2] <- predict(model1, newdata = data.frame(x=1))
}
ggplot() +
geom_line(data = data, aes(x = x, y = f), color = "black") +
geom_point(data = results1, aes(x = x, y = f_pred), color="red", shape=2)
results20 <- data.frame(x = 1, f_pred = 0)
for (i in 1:500) {
x <- seq(-3, 2, by = 0.1)
f <- true_relationship(x)
temp_observations <- f + rnorm(length(x), mean=0, sd=15)
model20 <- lm(temp_observations ~ poly(x, 20))
results20[i, 1] <- 1
results20[i, 2] <- predict(model20, newdata = data.frame(x=1))
}
ggplot() +
geom_line(data = data, aes(x = x, y = f), color = "black") +
geom_point(data = results20, aes(x = x, y = f_pred), color="orange", shape=2)
models <- vector("list", 25)
for (degree in 1:25) {
model <- lm(observations ~ poly(x, degree))
models[[degree]] <- model
}
results <- data.frame(degree = 1:25, rmse = 0)
for (degree in 1:25) {
predictions <- predict(models[[degree]], newdata = data.frame(x=x))
results[results$degree==degree, "rmse"] <-
sqrt((1/length(predictions))*sum((predictions-observations)^2))
}
ggplot() +
geom_line(data = results, aes(x = degree, y = rmse), color = "black")
results <- data.frame(degree = 1:25, rmse = 0)
for (degree in 1:25) {
predictions <- predict(models[[degree]], newdata = data.frame(x=x))
results[results$degree==degree, "rmse"] <-
sqrt((1/length(predictions))*sum((predictions-observations_new)^2))
}
ggplot() +
geom_line(data = results, aes(x = degree, y = rmse), color = "black")
model <- lm(observations ~ poly(x, 6))
predictions=predict(model, newdata = data.frame(x=x))
data = data.frame(x=x, f=f, predictions=predictions)
ggplot(data, aes(x=x)) +
geom_line(aes(y = f), color = "black") +
geom_line(aes(y = predictions), color = "red", linetype="solid")
library(dplyr)
library(caret)
library(NbClust)
install.packages("NbClust")
library(NbClust)
setwd("C:\\Users\\steve\\OneDrive\\Documents\\School\\DSE6211\\Week8")
data <- read.csv("lab_7_data.csv")
data <- read.csv("lab_8_data.csv")
training_ind <- createDataPartition(data$lodgepole_pine,
p = 0.75,
list = FALSE,
times = 1)
training_set <- data[training_ind, ]
test_set <- data[-training_ind, ]
top_20_soil_types <- training_set %>%
group_by(soil_type) %>%
summarise(count = n()) %>%
arrange(desc(count)) %>%
select(soil_type) %>%
top_n(20)
training_set$soil_type <- ifelse(training_set$soil_type %in%
top_20_soil_types$soil_type,
training_set$soil_type,
"other")
training_set$wilderness_area <-factor(training_set$wilderness_area)
training_set$soil_type <- factor(training_set$soil_type)
onehot_encoder <- dummyVars(~ wilderness_area + soil_type,
training_set[, c("wilderness_area",
"soil_type")],
levelsOnly = TRUE,
fullRank = TRUE)
onehot_enc_training <- predict(onehot_encoder,
training_set[, c("wilderness_area",
"soil_type")])
training_set <- cbind(training_set, onehot_enc_training)
test_set$soil_type <- ifelse(test_set$soil_type %in%
top_20_soil_types$soil_type,
test_set$soil_type,
"other")
test_set$wilderness_area <- factor(test_set$wilderness_area)
test_set$soil_type <- factor(test_set$soil_type)
onehot_enc_test <- predict(onehot_encoder, test_set[,
c("wilderness_area",
"soil_type")])
test_set <- cbind(test_set, onehot_enc_test)
test_set[, -c(11:13)] <- scale(test_set[, -c(11:13)],
center = apply(training_set[,
-c(11:13)],
2, mean),
scale = apply(training_set[,
-c(11:13)],
2, sd))
training_set[, -c(11:13)] <- scale(training_set[, -c(11:13)])
training_features <- array(data = unlist(training_set[,
-c(11:13)]),
dim = c(nrow(training_set), 33))
training_labels <- array(data = unlist(training_set[, 13]),
dim = c(nrow(training_set)))
test_features <- array(data = unlist(test_set[, -c(11:13)]),
dim = c(nrow(test_set), 33))
test_labels <- array(data = unlist(test_set[, 13]),
dim = c(nrow(test_set)))
View(data)
set.seed(123)
nc <- NbClust(training_features[sample(nrow(training_features), 1000), c(4, 6, 10)],
min.nc = 2, max.nc = 10, method = "kmeans")
km_clusters <- kmeans(training_features[, c(4, 6, 10)], centers = 4)
View(km_clusters)
cluster_number <- data.frame(cluster_number = km_clusters$cluster)
View(cluster_number)
View(cluster_number)
training_features <- cbind(training_features, cluster_number)
View(training_features)
head(training_features)
km_clusters$size
km_clusters$centers
aggregate(training_features, by=list(training_features$cluster_number), mean)
## set the random number seed to obtain reproducible results
set.seed(1234)
## load any libraries that will be used
library(NbClust)
data(wine, package = "rattle")
install.packages("rattle")
data(wine, package = "rattle")
head(wine)
## exclude the first column, since it contains the actual wine type
## we are going to try to identify the wine types using k-means clustering
## scale each of the variables to have a mean of zero and a standard deviation of one
wine_scaled <- scale(wine[, -1])
View(wine_scaled)
View(wine)
View(wine_scaled)
View(wine)
## use the NBClust function to search for the best number of clusters (between 2 and 15) for the kmeans algorithm
nc <- NbClust(wine_scaled, min.nc = 2, max.nc = 15, method = "kmeans")
barplot(table(nc$Best.n[1, ]), xlab = "Number of Clusters", ylab = "Number of Criteria",
main = "Number of Clusters Chosen by 26 Criteria")
fit.km <- kmeans(wine_scaled, centers = 3, nstart = 25)
## print the cluster sizes and cluster centers in a centroid table
fit.km$size
fit.km$centers
## create a data frame that contains the original data, as well as the cluster number for each observation
cluster_number <- data.frame(cluster_number = fit.km$cluster)
wine_clusters <- cbind(wine, cluster_number)
View(wine_clusters)
## explore the clusters to see if we can classify them based on the mean value for each variable within each cluster
aggregate(wine_clusters[, -1], by=list(wine_clusters$cluster_number), mean)
## compare the actual wine types with the clusters we found in the data - the clusters look very good!
ct.km <- table(wine$Type, fit.km$cluster)
ct.km
library(dplyr)
library(caret)
library(NbClust)
setwd("C:\\Users\\steve\\OneDrive\\Documents\\School\\DSE6211\\Week8")
data <- read.csv("lab_8_data.csv")
training_ind <- createDataPartition(data$lodgepole_pine,
p = 0.75,
list = FALSE,
times = 1)
training_set <- data[training_ind, ]
test_set <- data[-training_ind, ]
top_20_soil_types <- training_set %>%
group_by(soil_type) %>%
summarise(count = n()) %>%
arrange(desc(count)) %>%
select(soil_type) %>%
top_n(20)
training_set$soil_type <- ifelse(training_set$soil_type %in%
top_20_soil_types$soil_type,
training_set$soil_type,
"other")
training_set$wilderness_area <-factor(training_set$wilderness_area)
training_set$soil_type <- factor(training_set$soil_type)
onehot_encoder <- dummyVars(~ wilderness_area + soil_type,
training_set[, c("wilderness_area",
"soil_type")],
levelsOnly = TRUE,
fullRank = TRUE)
onehot_enc_training <- predict(onehot_encoder,
training_set[, c("wilderness_area",
"soil_type")])
training_set <- cbind(training_set, onehot_enc_training)
test_set$soil_type <- ifelse(test_set$soil_type %in%
top_20_soil_types$soil_type,
test_set$soil_type,
"other")
test_set$wilderness_area <- factor(test_set$wilderness_area)
test_set$soil_type <- factor(test_set$soil_type)
onehot_enc_test <- predict(onehot_encoder, test_set[,
c("wilderness_area",
"soil_type")])
test_set <- cbind(test_set, onehot_enc_test)
test_set[, -c(11:13)] <- scale(test_set[, -c(11:13)],
center = apply(training_set[,
-c(11:13)],
2, mean),
scale = apply(training_set[,
-c(11:13)],
2, sd))
training_set[, -c(11:13)] <- scale(training_set[, -c(11:13)])
training_features <- array(data = unlist(training_set[,
-c(11:13)]),
dim = c(nrow(training_set), 33))
training_labels <- array(data = unlist(training_set[, 13]),
dim = c(nrow(training_set)))
test_features <- array(data = unlist(test_set[, -c(11:13)]),
dim = c(nrow(test_set), 33))
test_labels <- array(data = unlist(test_set[, 13]),
dim = c(nrow(test_set)))
set.seed(123)
nc <- NbClust(training_features[sample(nrow(training_features), 1000), c(4, 6, 10)],
min.nc = 2, max.nc = 10, method = "kmeans")
km_clusters <- kmeans(training_features[, c(4, 6, 10)], centers = 4)
cluster_number <- data.frame(cluster_number = km_clusters$cluster)
training_features <- cbind(training_features, cluster_number)
head(training_features)
View(training_features)
km_clusters$size
km_clusters$centers
aggregate(training_features[,-c(11:13)], by=list(training_features$cluster_number), mean)
## set the random number seed to obtain reproducible results
set.seed(1234)
## load any libraries that will be used
library(NbClust)
data(wine, package = "rattle")
head(wine)
wine_scaled <- scale(wine[, -1])
head(wine_scaled)
## use the NBClust function to search for the best number of clusters (between 2 and 15) for the kmeans algorithm
nc <- NbClust(wine_scaled, min.nc = 2, max.nc = 15, method = "kmeans")
barplot(table(nc$Best.n[1, ]), xlab = "Number of Clusters", ylab = "Number of Criteria",
main = "Number of Clusters Chosen by 26 Criteria")
barplot(table(nc$Best.n[1, ]), xlab = "Number of Clusters", ylab = "Number of Criteria",
main = "Number of Clusters Chosen by 26 Criteria")
fit.km <- kmeans(wine_scaled, centers = 3, nstart = 25)
## print the cluster sizes and cluster centers in a centroid table
fit.km$size
fit.km$centers
## create a data frame that contains the original data, as well as the cluster number for each observation
cluster_number <- data.frame(cluster_number = fit.km$cluster)
wine_clusters <- cbind(wine, cluster_number)
View(wine_clusters)
View(wine_clusters)
