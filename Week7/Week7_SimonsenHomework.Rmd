---
title: "Week7_SimonsenHomework"
author: "Steven Simonsen"
date: "2024-10-11"
output: pdf_document
---

```{r}
library(dplyr)
library(caret)
library(reticulate)
library(tensorflow)
library(keras3)
library(MESS)

setwd("C:\\Users\\steve\\OneDrive\\Documents\\School\\DSE6211\\Week7")

data <- read.csv("lab_7_data.csv")

training_ind <- createDataPartition(data$lodgepole_pine,
                                    p = 0.75,
                                    list = FALSE,
                                    times = 1)

training_set <- data[training_ind, ]
test_set <- data[-training_ind, ]

top_20_soil_types <- training_set %>%
  group_by(soil_type) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  select(soil_type) %>%
  top_n(20)

training_set$soil_type <- ifelse(training_set$soil_type %in%
                                   top_20_soil_types$soil_type,
                                training_set$soil_type,
                                "other")

training_set$wilderness_area <-factor(training_set$wilderness_area)
training_set$soil_type <- factor(training_set$soil_type)

onehot_encoder <- dummyVars(~ wilderness_area + soil_type,
                            training_set[, c("wilderness_area",
                                             "soil_type")],
                            levelsOnly = TRUE,
                            fullRank = TRUE)


onehot_enc_training <- predict(onehot_encoder,
                               training_set[, c("wilderness_area",
                                                "soil_type")])

training_set <- cbind(training_set, onehot_enc_training)

test_set$soil_type <- ifelse(test_set$soil_type %in%
                               top_20_soil_types$soil_type,
                             test_set$soil_type,
                             "other")

test_set$wilderness_area <- factor(test_set$wilderness_area)
test_set$soil_type <- factor(test_set$soil_type)

onehot_enc_test <- predict(onehot_encoder, test_set[,
                                              c("wilderness_area",
                                                "soil_type")])

test_set <- cbind(test_set, onehot_enc_test)

test_set[, -c(11:13)] <- scale(test_set[, -c(11:13)],
                               center = apply(training_set[,
                                                        -c(11:13)],
                                              2, mean),
                               scale = apply(training_set[,
                                                        -c(11:13)],
                                             2, sd))

training_set[, -c(11:13)] <- scale(training_set[, -c(11:13)])

training_features <- array(data = unlist(training_set[,
                                                      -c(11:13)]),
                           dim = c(nrow(training_set), 33))

training_labels <- array(data = unlist(training_set[, 13]),
                         dim = c(nrow(training_set)))

test_features <- array(data = unlist(test_set[, -c(11:13)]),
                       dim = c(nrow(test_set), 33))

test_labels <- array(data = unlist(test_set[, 13]),
                     dim = c(nrow(test_set)))

pca_results <- prcomp(training_features[, 1:10])
summary(pca_results)

screeplot(pca_results, type = "line")

training_rotated <- as.matrix(training_features[, 1:10]) %*%
  pca_results$rotation

training_features <- cbind(training_features, training_rotated[,
                                                            1:6])

test_rotated <- as.matrix(test_features[, 1:10]) %*%
  pca_results$rotation

test_features <- cbind(test_features, test_rotated[, 1:6])


use_virtualenv("my_tf_workspace")

model <- keras_model_sequential() %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dense(units = 25, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")


compile(model,
        optimizer = "rmsprop",
        loss = "binary_crossentropy",
        metrics = "accuracy")

history <- fit(model, training_features, training_labels,
               epochs = 40, batch_size = 512, 
               validation_split = 0.33)

predictions <- predict(model, test_features)

test_set$p_prob <- predictions[, 1]

##### ROC curve
roc_data <- data.frame(threshold=seq(1,0,-0.01), fpr=0, tpr=0)

for (i in roc_data$threshold) {
  over_threshold <- test_set[test_set$p_prob >= i, ]
  fpr <- sum(over_threshold$lodgepole_pine==0)/
    sum(test_set$lodgepole_pine==0)
  roc_data[roc_data$threshold==i, "fpr"] <- fpr
  tpr <- sum(over_threshold$lodgepole_pine==1)/
    sum(test_set$lodgepole_pine==1)
  roc_data[roc_data$threshold==i, "tpr"] <- tpr
}

ggplot() +
  geom_line(data = roc_data, aes(x = fpr, y = tpr, color =
                                   threshold), size = 2) +
  scale_color_gradientn(colors = rainbow(3)) +
  geom_abline(intercept = 0, slope = 1, lty = 2) +
  geom_point(data = roc_data[seq(1, 101, 10), ], aes(x = fpr, y =
                                                       tpr)) +
  geom_text(data = roc_data[seq(1, 101, 10), ],
            aes(x = fpr, y = tpr, label = threshold, hjust = 1.2,
                vjust = -0.2))

##### AUC
auc <- auc(x = roc_data$fpr, y = roc_data$tpr, type = "spline")
auc
```

# Exercises
Hint for all exercises: see the synchronous live session slides.

1) What is the main difference between unsupervised and supervised learning?

In supervised learning, each observation is associated with a label. In unsupervised learning, there are no labels associated with the observations.

2) Is centering required for PCA? Is scaling required for PCA? Explain your answers.

Centering is always required for PCA since PCs are unit vectors originating from the origin. Scaling is almost always required to avoid PCA finding the features with the most variance. The exception is if numerical features are measured on similar scales and we want features with larger variance to have more imporatance.

3) After running the code above, run the following code to obtain the PCA feature loadings. Copy and paste the output. Which feature has the strongest influence on the first PC? Which feature has the
weakest influence on the first PC?

```{r}
pca_results$rotation
```
The feature with the strongest influence on the first PC is feature number 9 since this is the feature with the largest absolute value, and the feature with the weakest influence on the first PC is feature number 10 since this has the smallest absolute value.

