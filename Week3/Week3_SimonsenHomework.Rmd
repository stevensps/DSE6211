---
title: "Week3_SimonsenHomework"
author: "Steven Simonsen"
date: "2024-09-15"
output: pdf_document
---

```{r}
library(dplyr)
library(caret)

data <- read.csv("lab_3_data.csv")

training_ind <- createDataPartition(data$lodgepole_pine,
                                    p = 0.75,
                                    list = FALSE,
                                    times = 1)

training_set <- data[training_ind, ]
test_set <- data[-training_ind, ]

unique(training_set$wilderness_area)
unique(training_set$soil_type)


top_20_soil_types <- training_set %>%
  group_by(soil_type) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  select(soil_type) %>%
  top_n(20)

training_set$soil_type <- ifelse(training_set$soil_type %in% top_20_soil_types$soil_type,
                                 training_set$soil_type,
                                 "other")

training_set$wilderness_area <- factor(training_set$wilderness_area)
training_set$soil_type <- factor(training_set$soil_type)

class(training_set$wilderness_area)
class(training_set$soil_type)

levels(training_set$wilderness_area)
levels(training_set$soil_type)

onehot_encoder <- dummyVars(~ wilderness_area + soil_type,
                            training_set[, c("wilderness_area", "soil_type")],
                            levelsOnly = TRUE,
                            fullRank = TRUE)


onehot_enc_training <- predict(onehot_encoder,
                               training_set[, c("wilderness_area", "soil_type")])


training_set <- cbind(training_set, onehot_enc_training)



test_set$soil_type <- ifelse(test_set$soil_type %in% top_20_soil_types$soil_type,
                             test_set$soil_type,
                             "other")


test_set$wilderness_area <- factor(test_set$wilderness_area)
test_set$soil_type <- factor(test_set$soil_type)

onehot_enc_test <- predict(onehot_encoder, test_set[, c("wilderness_area", "soil_type")])

test_set <- cbind(test_set, onehot_enc_test)

test_set[, -c(11:13)] <- scale(test_set[, -c(11:13)],
                               center = apply(training_set[, -c(11:13)], 2, mean),
                               scale = apply(training_set[, -c(11:13)], 2, sd))

training_set[, -c(11:13)] <- scale(training_set[, -c(11:13)])


training_features <- array(data = unlist(training_set[, -c(11:13)]),
                           dim = c(nrow(training_set), 33))

training_labels <- array(data = unlist(training_set[, 13]),
                         dim = c(nrow(training_set)))
                         
test_features <- array(data = unlist(test_set[, -c(11:13)]),
                       dim = c(nrow(test_set), 33))

test_labels <- array(data = unlist(test_set[, 13]),
                     dim = c(nrow(test_set)))
```

#Exercises

1) After working through all of the code in the lab, run ‘head(training_features)’ and ‘head(test_features)’. Copy and paste the output.

```{r}
head(training_features)

head(test_features)
```

2) What is the rank of the tensor ‘training_features’? What is the shape of ‘training_features’? How many dimensions does ‘training_features’ have along the second axis?

```{r}
dim(training_features)
```

The rank of the tensor 'training_features' is rank 2. The shape is (6537, 33). The 'training_features' tensor has 33 dimensions along its second axis.


3) State two situations where scaling the numerical variables is important.

First, it is important to scale numerical variables when using machine learning methods that rely on gradient-descent for optimization (such as neural networks) for purposes of model stability, convergence speed, and to be sure there is equal contribution of features.

The second situation where it is important to scale numerical variables is when a model is built upon the notion of distance (such as k-means clustering, principal component analysis, or k-nearest neighbor). The reason for this is to prevent one numerical variable
from dominating the others simply because they are measured in different units (e.g., meters vs miles).






